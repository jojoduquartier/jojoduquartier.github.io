{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Josiah's Python Code Snippets About me My name is Josiah Hounyo and I am Data Scientist with a background in Mathematics. I have a B.S. from The Ohio State University and a Professional masters from Michigan State. I love coding in general and python is my main go to language. I learned Haskell before python and although I never use Haskell, it is the programming language I love the most. Project Intent I use Python on a daily basis for both work and personal purposes. Every once in a while I come up with reusable code that I end up using for multiple projects. Sometimes these bits of code prove useful for colleagues and friends. I have benefitted from many people sharing their code snippets online so this is a way for paying it forward. The snippets shared here are meant to be very simple and applicable in different situation; they are not data-science specific. If you have any suggestions or find issues with the snippets please provide information by commenting on the page; I welcome every opportunity to learn and improve. Connect GitHub All these posts represent my own (personal) opinion on the topics I cover. \u21a9 Non-native python packages will be used in the snippets, if I do not provide a link to the packages, there will certainly be easy to find on PyPI . \u21a9","title":"Home"},{"location":"#welcome-to-josiahs-python-code-snippets","text":"","title":"Welcome to Josiah's Python Code Snippets"},{"location":"#about-me","text":"My name is Josiah Hounyo and I am Data Scientist with a background in Mathematics. I have a B.S. from The Ohio State University and a Professional masters from Michigan State. I love coding in general and python is my main go to language. I learned Haskell before python and although I never use Haskell, it is the programming language I love the most.","title":"About me"},{"location":"#project-intent","text":"I use Python on a daily basis for both work and personal purposes. Every once in a while I come up with reusable code that I end up using for multiple projects. Sometimes these bits of code prove useful for colleagues and friends. I have benefitted from many people sharing their code snippets online so this is a way for paying it forward. The snippets shared here are meant to be very simple and applicable in different situation; they are not data-science specific. If you have any suggestions or find issues with the snippets please provide information by commenting on the page; I welcome every opportunity to learn and improve.","title":"Project Intent"},{"location":"#connect","text":"GitHub All these posts represent my own (personal) opinion on the topics I cover. \u21a9 Non-native python packages will be used in the snippets, if I do not provide a link to the packages, there will certainly be easy to find on PyPI . \u21a9","title":"Connect"},{"location":"snippets/close_db_connections/","text":"Close DB Connections Nearly every data scientist work(ed)s on a project that requires connection to a database (DB). It is always a good idea to close DB connections after reading/writing the data you need. SQLAlchemy has a dispose method for DB engines as well as objects to handle transactions that release resources automatically. Every DB API in python has a method to close connections; CXOracle has a close method for example. When dealing with different databases, in order to not repeat myself and make sure I always close connections, I tend to use a snippet like this: The Snippet import sqlite3 import contextlib from sqlalchemy import create_engine @contextlib . contextmanager def db_connections (): # the engines created here could all be Oracle, MSSql, Snowflake etc. try : # connect to some in-memory engine sales_db = create_engine ( 'sqlite:///' ) # another in-memory engine manufacturing_db = sqlite3 . connect ( ':memory:' ) # yield results yield sales_db , manufacturing_db except Exception as e : # handle exceptions etc pass finally : sales_db . dispose () manufacturing_db . close () Context Managers are very useful in python when it comes to providing and releasing resources (like opening a file to read from and releasing it for the next task). There is a utility decorator that makes it easy to turn your existing functions into context-managers and that is what I used for this snippet. An Example with db_connections () as ( sales_db , manufacturing_db ): # do something like # pd.read_sql(..., sales_db) # pd.read_sql(..., manufacturing_db) pass Thank you!","title":"Close DB Connections"},{"location":"snippets/close_db_connections/#close-db-connections","text":"Nearly every data scientist work(ed)s on a project that requires connection to a database (DB). It is always a good idea to close DB connections after reading/writing the data you need. SQLAlchemy has a dispose method for DB engines as well as objects to handle transactions that release resources automatically. Every DB API in python has a method to close connections; CXOracle has a close method for example. When dealing with different databases, in order to not repeat myself and make sure I always close connections, I tend to use a snippet like this:","title":"Close DB Connections"},{"location":"snippets/close_db_connections/#the-snippet","text":"import sqlite3 import contextlib from sqlalchemy import create_engine @contextlib . contextmanager def db_connections (): # the engines created here could all be Oracle, MSSql, Snowflake etc. try : # connect to some in-memory engine sales_db = create_engine ( 'sqlite:///' ) # another in-memory engine manufacturing_db = sqlite3 . connect ( ':memory:' ) # yield results yield sales_db , manufacturing_db except Exception as e : # handle exceptions etc pass finally : sales_db . dispose () manufacturing_db . close () Context Managers are very useful in python when it comes to providing and releasing resources (like opening a file to read from and releasing it for the next task). There is a utility decorator that makes it easy to turn your existing functions into context-managers and that is what I used for this snippet.","title":"The Snippet"},{"location":"snippets/close_db_connections/#an-example","text":"with db_connections () as ( sales_db , manufacturing_db ): # do something like # pd.read_sql(..., sales_db) # pd.read_sql(..., manufacturing_db) pass Thank you!","title":"An Example"},{"location":"snippets/logging_decorator/","text":"Try - Catch Exception Handling on Multiple Functions I often find myself splitting my code across modules each with their functions that I use in some sort of a main script or function for my projects. I like to log the progress of my code when it is deployed and I also like to keep one single log per project (when this makes sense obviously). When a function I imported is being used by the main function/script, I try to wrap it in some try/except (see below code block) block to make sure I catch any unexpected failure. try : # call the function except : # do something and log it finally : # release resources In the spirit of DRY , I started using this simple decorator for my functions so that I can avoid wrapping all functions with the try/except block. The Snippet from functools import wraps from loguru._logger import Logger def try_catch ( logger : Logger ): \"\"\" Decorate a function with the module logger to not just track errors but catch errors on any function without repeating a try:...except:... block :param logger: a logger object, could be a python native logger as well \"\"\" def proper_decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : output = func ( * args , ** kwargs ) return output except Exception as e : logger . exception ( f \"Failure in { func . __name__ } \" ) raise e return wrapper return proper_decorator I use the wraps decorator from functools so that I can use the function's actual name in my traceback. An Example from loguru import logger # set the logger handle etc. here @try_catch ( logger ) def my_function ( numpy_array , user_constant ): \"\"\" Just a 1-D Numpy arrays divided by some constant. P.S. this example is designed to fail with constant 0 i.e. I omit the ZeroDivision check \"\"\" reciprocal = 1 / user_constant return reciprocal * numpy_array # try import numpy as np my_function ( np . array ([ 0 , 1 ]), 2 ) my_function ( np . array ([ 0 , 1 ]), 0 ) # Inspect the log and traceback When my_function fails with user providing 0 for the constant, I want to see the full traceback and the decorator handles that. I would suggest using this for functions dealing with databases or external communication that can easily fail unexpectedly. Thank you!","title":"Try/Catch Decorator"},{"location":"snippets/logging_decorator/#try-catch","text":"","title":"Try - Catch"},{"location":"snippets/logging_decorator/#exception-handling-on-multiple-functions","text":"I often find myself splitting my code across modules each with their functions that I use in some sort of a main script or function for my projects. I like to log the progress of my code when it is deployed and I also like to keep one single log per project (when this makes sense obviously). When a function I imported is being used by the main function/script, I try to wrap it in some try/except (see below code block) block to make sure I catch any unexpected failure. try : # call the function except : # do something and log it finally : # release resources In the spirit of DRY , I started using this simple decorator for my functions so that I can avoid wrapping all functions with the try/except block.","title":"Exception Handling on Multiple Functions"},{"location":"snippets/logging_decorator/#the-snippet","text":"from functools import wraps from loguru._logger import Logger def try_catch ( logger : Logger ): \"\"\" Decorate a function with the module logger to not just track errors but catch errors on any function without repeating a try:...except:... block :param logger: a logger object, could be a python native logger as well \"\"\" def proper_decorator ( func ): @wraps ( func ) def wrapper ( * args , ** kwargs ): try : output = func ( * args , ** kwargs ) return output except Exception as e : logger . exception ( f \"Failure in { func . __name__ } \" ) raise e return wrapper return proper_decorator I use the wraps decorator from functools so that I can use the function's actual name in my traceback.","title":"The Snippet"},{"location":"snippets/logging_decorator/#an-example","text":"from loguru import logger # set the logger handle etc. here @try_catch ( logger ) def my_function ( numpy_array , user_constant ): \"\"\" Just a 1-D Numpy arrays divided by some constant. P.S. this example is designed to fail with constant 0 i.e. I omit the ZeroDivision check \"\"\" reciprocal = 1 / user_constant return reciprocal * numpy_array # try import numpy as np my_function ( np . array ([ 0 , 1 ]), 2 ) my_function ( np . array ([ 0 , 1 ]), 0 ) # Inspect the log and traceback When my_function fails with user providing 0 for the constant, I want to see the full traceback and the decorator handles that. I would suggest using this for functions dealing with databases or external communication that can easily fail unexpectedly. Thank you!","title":"An Example"},{"location":"snippets/organize_db_connections/","text":"Organize DB Credentials This is somewhat a shameless plug for a python package I developed \ud83d\ude05 Some data scientists (especially those not working on cloud platforms) cound find themselves with different database credentials from Oracle , Postgre , MySQL dialects. By DB credentials, I am referring to everything from the host address, the port number all the way to the username and passwords needed to establish connection to the database. I was once asked what was the best way to manage said credentials. The best way would certainly be to use some sort of key management system. Without such system, for quick and safe data exploration I found it useful to use json files like this: General DB Info - store file on protected drive { \"oracle\" : { \"my_database_1\" : { \"host\" : \"localhost:database1\" , \"port\" : \"1521\" , }, \"my_database_2\" : { \"host\" : \"localhost:database2\" , \"port\" : \"1521\" , } }, \"mysql\" : { \"my_database_1\" : { \"host\" : \"localhost:database1\" , \"port\" : \"3306\" , }, \"my_database_2\" : { \"host\" : \"localhost:database2\" , \"port\" : \"3306\" , } } } Credentials - encrypt credentials first Username and Passwords are sensitive info so I use the Cryptography package to encrypt them and store the file and save them on a safe drive { \"oracle\" : { \"my_database_1\" : { \"username\" : \"encrypted\" , \"port\" : \"encrypted\" , }, \"my_database_2\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , } }, \"mysql\" : { \"my_database_1\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , }, \"my_database_2\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , } } } The encryption key is stored somewhere else to make it harder to decypt the data. With these files, I can easily separate my Oracle databases from MySQL databases etc. But there's room for improvement; the code to connect to the databases is often the same and I would have to repeat the same thing over and over in jupyter notebooks or exploratory scripts. In order to avoid repetition, I created this package: dsdbmanager where dsdb stands for data science database . I will not speak in detail of the package but it allows me to effortlessly do things like: Add any new database to the file directly with functionality from the package Easily connect to any specific database I want with simple commands like: import pandas as pd from dsdbmanager import oracle oracle_databases = oracle () oracle_database_1 = oracle_databases . my_database_1 ( connect_only = True , schema = 'some_schema' ) pd . read_sql ( \"select * from some table\" , oracle_database_1 . sqlalchemy_engine ) See all tables and views available in a schema of my database See metadata on number of rows and all columns for any table/view in the schema When my projects move to production though, I do not use this package as we have ways of handling database credentials but this package has made my life very easy as I can quickly and effortlessly connect to databses and quickly check for things to debug issues. Hopefully you try this package out and find it useful! If you have any questions, please ask them in the comments below. The idea to develop a package like this was conceived while having a nice beer with a colleague and friend Tyler . \u21a9 The package currentl supports Oracle, MySQL, MSSQL, Teradata and Snowflake Connections. Contributors are more than welcome to improve the package. \u21a9","title":"Organize DB Credentials"},{"location":"snippets/organize_db_connections/#organize-db-credentials","text":"This is somewhat a shameless plug for a python package I developed \ud83d\ude05 Some data scientists (especially those not working on cloud platforms) cound find themselves with different database credentials from Oracle , Postgre , MySQL dialects. By DB credentials, I am referring to everything from the host address, the port number all the way to the username and passwords needed to establish connection to the database. I was once asked what was the best way to manage said credentials. The best way would certainly be to use some sort of key management system. Without such system, for quick and safe data exploration I found it useful to use json files like this:","title":"Organize DB Credentials"},{"location":"snippets/organize_db_connections/#general-db-info-store-file-on-protected-drive","text":"{ \"oracle\" : { \"my_database_1\" : { \"host\" : \"localhost:database1\" , \"port\" : \"1521\" , }, \"my_database_2\" : { \"host\" : \"localhost:database2\" , \"port\" : \"1521\" , } }, \"mysql\" : { \"my_database_1\" : { \"host\" : \"localhost:database1\" , \"port\" : \"3306\" , }, \"my_database_2\" : { \"host\" : \"localhost:database2\" , \"port\" : \"3306\" , } } }","title":"General DB Info - store file on protected drive"},{"location":"snippets/organize_db_connections/#credentials-encrypt-credentials-first","text":"Username and Passwords are sensitive info so I use the Cryptography package to encrypt them and store the file and save them on a safe drive { \"oracle\" : { \"my_database_1\" : { \"username\" : \"encrypted\" , \"port\" : \"encrypted\" , }, \"my_database_2\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , } }, \"mysql\" : { \"my_database_1\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , }, \"my_database_2\" : { \"host\" : \"encrypted\" , \"port\" : \"encrypted\" , } } } The encryption key is stored somewhere else to make it harder to decypt the data. With these files, I can easily separate my Oracle databases from MySQL databases etc. But there's room for improvement; the code to connect to the databases is often the same and I would have to repeat the same thing over and over in jupyter notebooks or exploratory scripts. In order to avoid repetition, I created this package: dsdbmanager where dsdb stands for data science database . I will not speak in detail of the package but it allows me to effortlessly do things like: Add any new database to the file directly with functionality from the package Easily connect to any specific database I want with simple commands like: import pandas as pd from dsdbmanager import oracle oracle_databases = oracle () oracle_database_1 = oracle_databases . my_database_1 ( connect_only = True , schema = 'some_schema' ) pd . read_sql ( \"select * from some table\" , oracle_database_1 . sqlalchemy_engine ) See all tables and views available in a schema of my database See metadata on number of rows and all columns for any table/view in the schema When my projects move to production though, I do not use this package as we have ways of handling database credentials but this package has made my life very easy as I can quickly and effortlessly connect to databses and quickly check for things to debug issues. Hopefully you try this package out and find it useful! If you have any questions, please ask them in the comments below. The idea to develop a package like this was conceived while having a nice beer with a colleague and friend Tyler . \u21a9 The package currentl supports Oracle, MySQL, MSSQL, Teradata and Snowflake Connections. Contributors are more than welcome to improve the package. \u21a9","title":"Credentials - encrypt credentials first"},{"location":"snippets/use_single_dispatch/","text":"Same Function Name - Different Data Types Context Whether you are writing code to process data or build machine learning models, you will find yourself is situations where you want to do one thing but on different structures like a numpy array, a pandas dataframe etc. Say you have to write a function to randomly sample data from a structure that could be either a numpy array or a pandas dataframe. You could create two different functions def sample_numpy ( data , n : int = 10 ): pass def sample_dataframe ( data , n : int = 10 ): pass You could write one function and check the type of the function import numpy as np import pandas as pd def sample ( data , n : int = 10 ): if isinstance ( data , np . ndarray ): # do something pass if isinstance ( data , pd . DataFrame ): # do something pass raise NotImplementedError () If you go with the first approach and need to handle more data types, you can easily end up with many functions and tracking the names would be cumbersome. If you went with the second approach, the function will quickly span multiple lines. There is a better approach, one that allows the use the exact same function name and split the functions so that you actually handle each data type separately (as if you wrote different functions). Enters singledispatch ! I highly recommend reading on this decorator and use it whenever you can. The Snippet import numpy as np import pandas as pd from functools import singledispatch @singledispatch def sample ( data , n : int = 10 ): raise NotImplementedError ( f \"Not yet implemented for { type ( data ) } type\" ) @sample . register ( np . ndarray ) def v1 ( data : np . ndarray , n : int = 10 ): # we want replacement is n is bigger than the # observations indices = np . random . choice ( data . shape [ 0 ], n , replace = ( n > len ( data ))) return data [ indices ] . copy () @sample . register ( pd . DataFrame ) def v2 ( data : pd . DataFrame , n : int = 10 ): return data . sample ( n , replace = ( n > len ( data ))) An Example Assuming you have the code from the block above df = pd . read_csv ( \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/master/sklearn/datasets/data/iris.csv\" ) numpy_representation = df . values # try it on the pandas dataframe sample ( df ) sample ( df , 1000 ) # should have replacement # try it on the numpy array from the dataframe sample ( numpy_representation ) sample ( numpy_representation , 1000 ) # now try it on something you have not implemented it for sample ([ 1 , 2 , 3 , 4 , 5 ]) Please read up on this cool decorator from python. It became available with python 3.4 and is (in my opinion) very useful! \u21a9 If you use python 3.8+, checkout the singledispatchmethod for dealing with class methods. \u21a9","title":"Same Function Name - Different Data Types"},{"location":"snippets/use_single_dispatch/#same-function-name-different-data-types","text":"","title":"Same Function Name - Different Data Types"},{"location":"snippets/use_single_dispatch/#context","text":"Whether you are writing code to process data or build machine learning models, you will find yourself is situations where you want to do one thing but on different structures like a numpy array, a pandas dataframe etc. Say you have to write a function to randomly sample data from a structure that could be either a numpy array or a pandas dataframe. You could create two different functions def sample_numpy ( data , n : int = 10 ): pass def sample_dataframe ( data , n : int = 10 ): pass You could write one function and check the type of the function import numpy as np import pandas as pd def sample ( data , n : int = 10 ): if isinstance ( data , np . ndarray ): # do something pass if isinstance ( data , pd . DataFrame ): # do something pass raise NotImplementedError () If you go with the first approach and need to handle more data types, you can easily end up with many functions and tracking the names would be cumbersome. If you went with the second approach, the function will quickly span multiple lines. There is a better approach, one that allows the use the exact same function name and split the functions so that you actually handle each data type separately (as if you wrote different functions). Enters singledispatch ! I highly recommend reading on this decorator and use it whenever you can.","title":"Context"},{"location":"snippets/use_single_dispatch/#the-snippet","text":"import numpy as np import pandas as pd from functools import singledispatch @singledispatch def sample ( data , n : int = 10 ): raise NotImplementedError ( f \"Not yet implemented for { type ( data ) } type\" ) @sample . register ( np . ndarray ) def v1 ( data : np . ndarray , n : int = 10 ): # we want replacement is n is bigger than the # observations indices = np . random . choice ( data . shape [ 0 ], n , replace = ( n > len ( data ))) return data [ indices ] . copy () @sample . register ( pd . DataFrame ) def v2 ( data : pd . DataFrame , n : int = 10 ): return data . sample ( n , replace = ( n > len ( data )))","title":"The Snippet"},{"location":"snippets/use_single_dispatch/#an-example","text":"Assuming you have the code from the block above df = pd . read_csv ( \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/master/sklearn/datasets/data/iris.csv\" ) numpy_representation = df . values # try it on the pandas dataframe sample ( df ) sample ( df , 1000 ) # should have replacement # try it on the numpy array from the dataframe sample ( numpy_representation ) sample ( numpy_representation , 1000 ) # now try it on something you have not implemented it for sample ([ 1 , 2 , 3 , 4 , 5 ]) Please read up on this cool decorator from python. It became available with python 3.4 and is (in my opinion) very useful! \u21a9 If you use python 3.8+, checkout the singledispatchmethod for dealing with class methods. \u21a9","title":"An Example"}]}